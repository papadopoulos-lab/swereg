---
title: "Advanced Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Advanced Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(data.table)
```

## Introduction

This vignette demonstrates the complete swereg workflow for production-scale registry analysis.

### The Three-Stage Skeleton Concept

Think of building a skeleton like constructing a body: you create **'good bones'** (the time structure) and then attach the **'muscles'** (the data) systematically.

**The Three Stages:**

1. **skeleton1_create**: Raw data attachment stage
   - Build the time-structured framework ('good bones')
   - Attach raw registry data from multiple Swedish sources (NPR, LMED, SCB, cause of death)
   - Focus on data integration, not cleaning
   - Use memory-efficient batching for large populations

2. **skeleton2_clean**: Data cleaning and derivation stage  
   - Clean and standardize variables  
   - Create derived variables and clinical indicators
   - Handle missing data patterns and outliers
   - Prepare variables for analysis

3. **skeleton3_analyze**: Analysis preparation stage
   - Apply study-specific inclusion/exclusion criteria
   - Minimize to only rows/columns needed for the analysis
   - Create final analysis-ready datasets

**This approach scales to Swedish registry studies with hundreds of thousands of individuals.**

## Phase 1: skeleton1_create (Data Integration)

### Memory-Efficient Batching Strategy

For large studies, process data in batches to manage memory:

```{r}
# Setup for batched processing
BATCH_SIZE <- 50  # Small for demonstration
OUTPUT_DIR <- tempdir()  # Use temporary directory for vignette

# Load all fake data
data("fake_person_ids", package = "swereg")
data("fake_demographics", package = "swereg")
data("fake_annual_family", package = "swereg") 
data("fake_inpatient_diagnoses", package = "swereg")
data("fake_outpatient_diagnoses", package = "swereg")
data("fake_prescriptions", package = "swereg")
data("fake_cod", package = "swereg")

# Apply make_lowercase_names to all datasets
swereg::make_lowercase_names(fake_demographics)
swereg::make_lowercase_names(fake_annual_family)
swereg::make_lowercase_names(fake_inpatient_diagnoses)
swereg::make_lowercase_names(fake_outpatient_diagnoses)
swereg::make_lowercase_names(fake_prescriptions)
swereg::make_lowercase_names(fake_cod)

cat("Loaded datasets with lowercase names applied\n")
```

### Batch Processing Function

Create skeleton1_create for each batch:

```{r}
skeleton1_create_batch <- function(batch_ids, batch_number) {
  cat("Processing batch", batch_number, "with", length(batch_ids), "individuals\n")
  
  # Create skeleton for this batch
  skeleton <- swereg::create_skeleton(
    ids = batch_ids,
    date_min = "2015-01-01",
    date_max = "2018-12-31"  # Shorter period for demonstration
  )
  
  # Add demographics
  demographics_subset <- fake_demographics[lopnr %in% batch_ids]
  if (nrow(demographics_subset) > 0) {
    swereg::add_onetime(skeleton, demographics_subset, id_name = "lopnr")
  }
  
  # Add annual data
  annual_subset <- fake_annual_family[lopnr %in% batch_ids]
  if (nrow(annual_subset) > 0) {
    swereg::add_annual(skeleton, annual_subset, id_name = "lopnr", isoyear = 2015)
  }
  
  # Add diagnoses
  diagnoses_subset <- rbindlist(list(
    fake_inpatient_diagnoses[lopnr %in% batch_ids],
    fake_outpatient_diagnoses[lopnr %in% batch_ids]
  ), use.names = TRUE, fill = TRUE)
  
  if (nrow(diagnoses_subset) > 0) {
    swereg::add_diagnoses(
      skeleton,
      diagnoses_subset,
      id_name = "lopnr",
      diags = list(
        "depression" = c("F32", "F33"),
        "anxiety" = c("F40", "F41"),
        "gender_dysphoria" = c("F64"),
        "psychosis" = c("F20", "F25")
      )
    )
  }
  
  # Add prescriptions
  prescriptions_subset <- fake_prescriptions[p444_lopnr_personnr %in% batch_ids]
  if (nrow(prescriptions_subset) > 0) {
    swereg::add_rx(
      skeleton,
      prescriptions_subset,
      id_name = "p444_lopnr_personnr",
      rxs = list(
        "antidepressants" = c("N06A"),
        "antipsychotics" = c("N05A"),
        "hormones" = c("G03")
      )
    )
  }
  
  # Add cause of death
  cod_subset <- fake_cod[lopnr %in% batch_ids]
  if (nrow(cod_subset) > 0) {
    swereg::add_cods(
      skeleton,
      cod_subset,
      id_name = "lopnr",
      cods = list(
        "external_death" = c("X60", "X70"),
        "cardiovascular_death" = c("I21", "I22")
      )
    )
  }
  
  # Save batch
  output_file <- file.path(OUTPUT_DIR, paste0("skeleton1_batch_", batch_number, ".rds"))
  saveRDS(skeleton, output_file)
  
  cat("Saved skeleton1 batch", batch_number, ":", nrow(skeleton), "rows\n")
  return(output_file)
}

# Process first 100 individuals in 2 batches
ids_subset <- fake_person_ids[1:100]
id_batches <- split(ids_subset, ceiling(seq_along(ids_subset) / BATCH_SIZE))

skeleton1_files <- vector("character", length(id_batches))
for (i in seq_along(id_batches)) {
  skeleton1_files[i] <- skeleton1_create_batch(id_batches[[i]], i)
}

cat("skeleton1_create phase completed for", length(id_batches), "batches\n")
```

## Phase 2: skeleton2_clean (Data Cleaning)

After removing large datasets from memory, clean and derive variables:

```{r}
skeleton2_clean_batch <- function(batch_number) {
  cat("Cleaning batch", batch_number, "\n")
  
  # Load skeleton1 for this batch
  input_file <- file.path(OUTPUT_DIR, paste0("skeleton1_batch_", batch_number, ".rds"))
  skeleton <- readRDS(input_file)
  
  # CLEANING OPERATIONS (using only data within skeleton)
  
  # 1. Create age variable
  skeleton[, birth_year := as.numeric(substr(fodelseman, 1, 4))]
  skeleton[, age := isoyear - birth_year]
  
  # 2. Create mental health composite variables
  skeleton[, any_mental_health := depression | anxiety | psychosis]
  skeleton[, severe_mental_illness := psychosis | gender_dysphoria]
  
  # 3. Create medication concordance variables
  skeleton[, depression_treated := depression & antidepressants]
  skeleton[, psychosis_treated := psychosis & antipsychotics]
  
  # 4. Create life stage variables
  skeleton[, life_stage := fcase(
    age < 18, "child",
    age >= 18 & age < 65, "adult", 
    age >= 65, "elderly",
    default = "unknown"
  )]
  
  # 5. Create outcome variables
  skeleton[, death_any := external_death | cardiovascular_death]
  
  # 6. Filter to valid ages and reasonable time periods
  skeleton <- skeleton[age >= 0 & age <= 100]
  skeleton <- skeleton[isoyear >= 2015]  # Remove historical rows
  
  # 7. Create person-level summaries for annual data
  if (skeleton[, any(is_isoyear == TRUE)]) {
    skeleton[is_isoyear == TRUE, n_mental_health_year := sum(c(depression, anxiety, psychosis), na.rm = TRUE), by = .(id, isoyear)]
    skeleton[is_isoyear == TRUE, treatment_adherence := mean(c(depression_treated, psychosis_treated), na.rm = TRUE), by = .(id, isoyear)]
  }
  
  # 8. Create registry tag variables (simulate case-control study)
  skeleton[, register_tag := fcase(
    gender_dysphoria == TRUE, "case",
    id %% 3 == 0, "control_matched",
    default = "control_population"
  )]
  
  # 9. Create shared case variables (for matched studies)
  # Find first gender dysphoria diagnosis for cases
  gd_first <- skeleton[gender_dysphoria == TRUE & register_tag == "case", 
                       .(first_gd_year = min(isoyear, na.rm = TRUE)), 
                       by = .(id)]
  
  # Add to skeleton
  skeleton[gd_first, on = "id", first_gd_year := first_gd_year]
  
  # For controls, assign their matched case's first GD year (simplified)
  skeleton[register_tag != "case", first_gd_year := 2016]  # Simplified for demo
  
  # 10. Remove temporary variables
  skeleton[, c("fodelseman", "birth_year") := NULL]
  
  # Save cleaned skeleton
  output_file <- file.path(OUTPUT_DIR, paste0("skeleton2_batch_", batch_number, ".rds"))
  saveRDS(skeleton, output_file)
  
  cat("Cleaned batch", batch_number, ":", nrow(skeleton), "rows,", ncol(skeleton), "columns\n")
  return(output_file)
}

# Process all batches for skeleton2_clean
skeleton2_files <- vector("character", length(id_batches))
for (i in seq_along(id_batches)) {
  skeleton2_files[i] <- skeleton2_clean_batch(i)
}

cat("skeleton2_clean phase completed\n")
```

## Phase 3: skeleton3_analyze (Analysis Dataset)

Extract analysis-ready dataset with specific variables:

```{r}
skeleton3_analyze <- function(skeleton2_files) {
  cat("Creating analysis dataset from", length(skeleton2_files), "batches\n")
  
  # Load all cleaned batches
  all_batches <- vector("list", length(skeleton2_files))
  for (i in seq_along(skeleton2_files)) {
    skeleton <- readRDS(skeleton2_files[i])
    
    # Extract analysis variables (person-year level)
    analysis_data <- skeleton[
      is_isoyear == TRUE,  # Use annual rows only
      .(
        # Core identifiers
        id = id,
        year = isoyear,
        register_tag = first_non_na(register_tag),
        
        # Demographic variables
        age = first_non_na(age),
        life_stage = first_non_na(life_stage),
        
        # Outcome variables
        any_mental_health = as.logical(max(any_mental_health, na.rm = TRUE)),
        severe_mental_illness = as.logical(max(severe_mental_illness, na.rm = TRUE)),
        depression = as.logical(max(depression, na.rm = TRUE)),
        anxiety = as.logical(max(anxiety, na.rm = TRUE)),
        psychosis = as.logical(max(psychosis, na.rm = TRUE)),
        gender_dysphoria = as.logical(max(gender_dysphoria, na.rm = TRUE)),
        
        # Treatment variables
        antidepressants = as.logical(max(antidepressants, na.rm = TRUE)),
        antipsychotics = as.logical(max(antipsychotics, na.rm = TRUE)),
        hormones = as.logical(max(hormones, na.rm = TRUE)),
        
        # Derived variables
        depression_treated = as.logical(max(depression_treated, na.rm = TRUE)),
        psychosis_treated = as.logical(max(psychosis_treated, na.rm = TRUE)),
        
        # Mortality
        death_any = as.logical(max(death_any, na.rm = TRUE)),
        
        # Study design variables
        first_gd_year = first_non_na(first_gd_year),
        
        # Summary variables
        n_mental_health_year = first_non_na(n_mental_health_year),
        treatment_adherence = first_non_na(treatment_adherence)
      ),
      by = .(id, isoyear, register_tag)
    ]
    
    all_batches[[i]] <- analysis_data
  }
  
  # Combine all batches
  final_analysis <- rbindlist(all_batches)
  
  # Final analysis dataset preparation
  final_analysis[, follow_up_years := year - first_gd_year]
  final_analysis[, study_period := fcase(
    follow_up_years < 0, "before_index",
    follow_up_years == 0, "index_year", 
    follow_up_years > 0, "after_index"
  )]
  
  return(final_analysis)
}

# Create final analysis dataset
analysis_data <- skeleton3_analyze(skeleton2_files)

cat("Analysis dataset created:", nrow(analysis_data), "person-years\n")
cat("Variables:", ncol(analysis_data), "\n")
cat("Study population breakdown:\n")
print(table(analysis_data$register_tag))
```

## Analysis Dataset Summary

Your final skeleton3_analyze contains analysis-ready data:

```{r}
# Show structure
str(analysis_data)

# Example analysis: Depression prevalence by register tag
depression_summary <- analysis_data[, .(
  n_persons = .N,
  depression_prev = mean(depression, na.rm = TRUE),
  treatment_rate = mean(depression_treated[depression == TRUE], na.rm = TRUE)
), by = .(register_tag, study_period)]

print(depression_summary)

# Example: Mental health treatment patterns
treatment_summary <- analysis_data[any_mental_health == TRUE, .(
  antidepressant_use = mean(antidepressants, na.rm = TRUE),
  antipsychotic_use = mean(antipsychotics, na.rm = TRUE),
  hormone_use = mean(hormones, na.rm = TRUE),
  mean_age = mean(age, na.rm = TRUE)
), by = register_tag]

print(treatment_summary)
```

## Key Workflow Principles

### Memory Management
1. **Batch processing**: Process individuals in groups (50-100 per batch)
2. **Phase separation**: Remove large datasets after skeleton1_create
3. **File-based workflow**: Save/load batches to manage memory

### Data Cleaning Strategy
1. **Self-contained**: skeleton2_clean uses only data within skeleton
2. **Derived variables**: Create analysis variables from raw data
3. **Quality filters**: Remove invalid observations

### Analysis Dataset Design
1. **Person-time structure**: Typically person-years for epidemiological analysis
2. **Study design variables**: Support case-control, cohort designs
3. **Analysis-ready**: No further data processing needed

## Summary: The Three-Stage swereg Workflow

This complete pipeline demonstrates the **'good bones, then muscles'** approach:

1. **skeleton1_create**: Built the time-structured framework and attached raw Swedish registry data
2. **skeleton2_clean**: Cleaned variables and created derived clinical indicators  
3. **skeleton3_analyze**: Applied study criteria and created analysis-ready datasets

**Key Benefits:**
- **Modular approach**: Each stage has clear responsibilities
- **Scalable**: Handles Swedish national registry data with millions of individuals
- **Reproducible**: Systematic workflow with clear documentation
- **Memory-efficient**: Batching strategy for large populations
- **Analysis-ready output**: No further data processing needed for statistical analysis

This systematic approach ensures robust, reproducible epidemiological research using Swedish healthcare registries.

```{r, include=FALSE}
# Clean up temporary files
unlink(file.path(OUTPUT_DIR, "skeleton*.rds"))
```
